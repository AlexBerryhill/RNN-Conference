{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "7_J02d99nN_1"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVi3CaRIn47I"
      },
      "source": [
        "# Parse Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppt-qEtln4YB",
        "outputId": "27a85834-7d27-46f8-ae3b-28d52235bdfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 39855119 characters\n"
          ]
        }
      ],
      "source": [
        "# Load file data\n",
        "path_to_file = tf.keras.utils.get_file('conference.txt', 'https://github.com/AlexBerryhill/RNN-Conference/raw/main/data/talk_values.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KnUTK3jn_Dw",
        "outputId": "741d3e63-c918-45eb-facc-70c6453f42ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My dear brothers and sisters:\n",
            "We welcome you, and all those who hear and see on radio and television. We welcome you to the sessions of the 141st Annual General Conference of The Church of Jesus Chris\n"
          ]
        }
      ],
      "source": [
        "# Verify the first part of our data\n",
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRixkBoZoLBr",
        "outputId": "3a20e3fe-81b5-4fea-8dea-8b6fd43dd452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "112 unique characters\n",
            "['\\n', ' ', '!', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '¡', '¢', '©', '®', '°', '·', '½', '¿', 'æ', 'ø', '̀', '́', '̂', '̃', '̈', '̌', '̧', '–', '—', '‘', '’', '“', '”', '…', '™', 'ﬁ', '\\ufeff']\n"
          ]
        }
      ],
      "source": [
        "# Now we'll get a list of the unique characters in the file. This will form the\n",
        "# vocabulary of our network. There may be some characters we want to remove from this\n",
        "# set as we refine the network.\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "K68gIZiaoMq1"
      },
      "outputs": [],
      "source": [
        "# Next, we'll encode encode these characters into numbers so we can use them\n",
        "# with our neural network, then we'll create some mappings between the characters\n",
        "# and their numeric representations\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab))\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "\n",
        "# Here's a little helper function that we can use to turn a sequence of ids\n",
        "# back into a string:\n",
        "# turn them into a string:\n",
        "def text_from_ids(ids):\n",
        "  joinedTensor = tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "  return joinedTensor.numpy().decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv4hsydaoQTX",
        "outputId": "569545b8-6448-461b-f384-ce453c8fa1ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([49, 76, 79, 78, 66], dtype=int64)>"
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now we'll verify that they work, by getting the code for \"A\", and then looking\n",
        "# that up in reverse\n",
        "testids = ids_from_chars([\"T\", \"r\", \"u\", \"t\", \"h\"])\n",
        "testids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRZUQLiJoV7h",
        "outputId": "83e24cf7-7488-4f8a-a349-038a48543f74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'T', b'r', b'u', b't', b'h'], dtype=object)>"
            ]
          },
          "execution_count": 203,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars_from_ids(testids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "agUgBQVqoXzc",
        "outputId": "a3b50e57-335d-473f-87a6-6c35edb13576"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Truth'"
            ]
          },
          "execution_count": 204,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testString = text_from_ids( testids )\n",
        "testString"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDtDGT06obfR",
        "outputId": "4fc3a778-90c7-42c7-ce7f-5ccecebde7d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(39855119,), dtype=int64, numpy=array([42, 83,  2, ..., 14,  1,  1], dtype=int64)>"
            ]
          },
          "execution_count": 205,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First, create a stream of encoded integers from our text\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "UgVcMxsxod6A"
      },
      "outputs": [],
      "source": [
        "# Now, convert that into a tensorflow dataset\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "fGkC2bvvoffm"
      },
      "outputs": [],
      "source": [
        "# Finally, let's batch these sequences up into chunks for our training\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# This function will generate our sequence pairs:\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Call the function for every sequence in our list to create a new dataset\n",
        "# of input->target pairs\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgM632IdohE-",
        "outputId": "49fc87be-5f91-4a08-9b13-f98f32c9bcc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  My dear brothers and sisters:\n",
            "We welcome you, and all those who hear and see on radio and television\n",
            "--------\n",
            "Target:  y dear brothers and sisters:\n",
            "We welcome you, and all those who hear and see on radio and television.\n"
          ]
        }
      ],
      "source": [
        "# Verify our sequences\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print(\"Input: \", text_from_ids(input_example))\n",
        "    print(\"--------\")\n",
        "    print(\"Target: \", text_from_ids(target_example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17azNHwUopYY",
        "outputId": "7f254360-d9f8-41dd-dc79-15bdace160b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Finally, we'll randomize the sequences so that we don't just memorize the books\n",
        "# in the order they were written, then build a new streaming dataset from that.\n",
        "# Using a streaming dataset allows us to pass the data to our network bit by bit,\n",
        "# rather than keeping it all in memory. We'll set it to figure out how much data\n",
        "# to prefetch in the background.\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "Qxlp1T6Nosuf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "class ConferenceTextModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super(ConferenceTextModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                                   embeddings_regularizer=regularizers.l1(1e-4))\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                       return_sequences=True, \n",
        "                                       return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = [tf.zeros([x.shape[0], self.gru.units])]\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else: \n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "FM4Y8BkSo-9_"
      },
      "outputs": [],
      "source": [
        "# Create an instance of our model\n",
        "vocab_size=len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = ConferenceTextModel(vocab_size, embedding_dim, rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqMNqbN9pA4T",
        "outputId": "d5d35a6b-50e1-447d-8267-f30107c70d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 113) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Verify the output of our model is correct by running one sample through\n",
        "# This will also compile the model for us. This step will take a bit.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    states = tf.zeros([64, model.gru.units])\n",
        "    example_batch_predictions = model(input_example_batch, states=states)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsJtjF-apEQK",
        "outputId": "eb9cea30-fd1e-41e4-dc7b-955ed93b46a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "6165/6165 [==============================] - 56s 9ms/step - loss: 1.2653\n",
            "Epoch 2/1000\n",
            "6165/6165 [==============================] - 54s 9ms/step - loss: 1.0643\n",
            "Epoch 3/1000\n",
            "6165/6165 [==============================] - 57s 9ms/step - loss: 1.0288\n",
            "Epoch 4/1000\n",
            "6165/6165 [==============================] - 57s 9ms/step - loss: 1.0121\n",
            "Epoch 5/1000\n",
            "6165/6165 [==============================] - 57s 9ms/step - loss: 1.0031\n",
            "Epoch 6/1000\n",
            "6165/6165 [==============================] - 57s 9ms/step - loss: 0.9976\n",
            "Epoch 7/1000\n",
            "6165/6165 [==============================] - 57s 9ms/step - loss: 0.9943\n",
            "Epoch 8/1000\n",
            "6165/6165 [==============================] - 57s 9ms/step - loss: 0.9929\n",
            "Epoch 9/1000\n",
            "6165/6165 [==============================] - 57s 9ms/step - loss: 0.9925\n",
            "Epoch 10/1000\n",
            "6165/6165 [==============================] - 57s 9ms/step - loss: 0.9940\n",
            "Epoch 11/1000\n",
            "6165/6165 [==============================] - 57s 9ms/step - loss: 1.0050\n",
            "Epoch 12/1000\n",
            "6165/6165 [==============================] - 58s 9ms/step - loss: 1.0323\n",
            "Epoch 13/1000\n",
            "6165/6165 [==============================] - 57s 9ms/step - loss: 1.0025\n",
            "Epoch 14/1000\n",
            "6165/6165 [==============================] - 58s 9ms/step - loss: 1.0050\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping_callback = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(dataset, epochs=1000, callbacks=[early_stopping_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "mjq4IP0ApH5R"
      },
      "outputs": [],
      "source": [
        "# Here's the code we'll use to sample for us. It has some extra steps to apply\n",
        "# the temperature to the distribution, and to make sure we don't get empty\n",
        "# characters in our text. Most importantly, it will keep track of our model\n",
        "# state for us.\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return chars_from_ids(predicted_ids), states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "J7O5T3cdpJXs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I pray that you will bind yourselves unto the Holy Ghost and yet individually refreshing to magnify and know that He knows God’s love continue. It not only impartly responds to many people. Interestingly, we humbly move forward, for he has filled the burden at these twelve heant by the power of the Holy Ghost” to care for the Savior’s words, “No house that will heapeth softens and let the hope or amused in this life: but because this is even a man in the ways: that the devil shall witnesse unto their sins and receive the prophets to organize ourselves.\n",
            "The Tynathan Missoe State President was facing microruncipating for the pull in this way about active in their home.\n",
            "The flock of politely redeeming the Spirit of Christ stayed pains that declared as a perfect principle in the Bible Mission who the next the small grandmother Healing is a great faith and nature. The is current ever on all, as he became, “Seoth, and we will not fall into the lany of hated couple at the end of paying tithing\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of the character generator\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "# Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
        "# as its starting text\n",
        "states = None\n",
        "next_char = tf.constant(['I'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# Print the results formatted.\n",
        "print(result[0].numpy().decode('utf-8'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_23_layer_call_fn, gru_cell_23_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: conference_model2\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: conference_model2\\assets\n"
          ]
        }
      ],
      "source": [
        "model.save('conference_model2', save_format='tf')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
