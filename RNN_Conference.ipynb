{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "7_J02d99nN_1"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import preprocessing, regularizers\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVi3CaRIn47I"
      },
      "source": [
        "# Parse Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppt-qEtln4YB",
        "outputId": "27a85834-7d27-46f8-ae3b-28d52235bdfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 41879130 characters\n"
          ]
        }
      ],
      "source": [
        "# Load file data\n",
        "path_to_file = tf.keras.utils.get_file('rally2.txt', 'https://raw.githubusercontent.com/AlexBerryhill/RNN-Conference/main/data/rally/rally_speeches.txt')\n",
        "text1 = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "path_to_file = tf.keras.utils.get_file('poem2.txt', 'https://raw.githubusercontent.com/ChildL/RNN/main/poetry_combined.txt')\n",
        "text2 = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "path_to_file = tf.keras.utils.get_file('conference_text2.txt', 'https://raw.githubusercontent.com/AlexBerryhill/RNN-Conference/main/data/conference/conference_talks.txt')\n",
        "text3 = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "path_to_file = tf.keras.utils.get_file('seuss2.txt', 'https://raw.githubusercontent.com/ChildL/RNN/main/seuss_combined.txt')\n",
        "text4 = open(path_to_file, 'rb').read().decode(encoding='unicode_escape')\n",
        "text = text1 + text2 + text3\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KnUTK3jn_Dw",
        "outputId": "741d3e63-c918-45eb-facc-70c6453f42ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job together. And Merry Christmas, Michigan. Thank you, Michigan. What a victory we had in Michigan. What a\n"
          ]
        }
      ],
      "source": [
        "# Verify the first part of our data\n",
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRixkBoZoLBr",
        "outputId": "3a20e3fe-81b5-4fea-8dea-8b6fd43dd452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "92 unique characters\n",
            "['\\n', '\\r', ' ', '!', '\"', '$', '%', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', '–', '—', '‘', '’', '“', '”', '…']\n"
          ]
        }
      ],
      "source": [
        "# Now we'll get a list of the unique characters in the file. This will form the\n",
        "# vocabulary of our network. There may be some characters we want to remove from this\n",
        "# set as we refine the network.\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "K68gIZiaoMq1"
      },
      "outputs": [],
      "source": [
        "# Next, we'll encode encode these characters into numbers so we can use them\n",
        "# with our neural network, then we'll create some mappings between the characters \n",
        "# and their numeric representations\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab))\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "\n",
        "# Here's a little helper function that we can use to turn a sequence of ids\n",
        "# back into a string:\n",
        "# turn them into a string:\n",
        "def text_from_ids(ids):\n",
        "  joinedTensor = tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "  return joinedTensor.numpy().decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv4hsydaoQTX",
        "outputId": "569545b8-6448-461b-f384-ce453c8fa1ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([49, 76, 79, 78, 66], dtype=int64)>"
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now we'll verify that they work, by getting the code for \"A\", and then looking\n",
        "# that up in reverse\n",
        "testids = ids_from_chars([\"T\", \"r\", \"u\", \"t\", \"h\"])\n",
        "testids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRZUQLiJoV7h",
        "outputId": "83e24cf7-7488-4f8a-a349-038a48543f74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'T', b'r', b'u', b't', b'h'], dtype=object)>"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars_from_ids(testids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "agUgBQVqoXzc",
        "outputId": "a3b50e57-335d-473f-87a6-6c35edb13576"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Truth'"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testString = text_from_ids( testids )\n",
        "testString"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDtDGT06obfR",
        "outputId": "4fc3a778-90c7-42c7-ce7f-5ccecebde7d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(41879130,), dtype=int64, numpy=array([49, 66, 59, ...,  1,  2,  1], dtype=int64)>"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First, create a stream of encoded integers from our text\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "UgVcMxsxod6A"
      },
      "outputs": [],
      "source": [
        "# Now, convert that into a tensorflow dataset\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "fGkC2bvvoffm"
      },
      "outputs": [],
      "source": [
        "# Finally, let's batch these sequences up into chunks for our training\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# This function will generate our sequence pairs:\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Call the function for every sequence in our list to create a new dataset\n",
        "# of input->target pairs\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgM632IdohE-",
        "outputId": "49fc87be-5f91-4a08-9b13-f98f32c9bcc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job tog\n",
            "--------\n",
            "Target:  hank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job toge\n"
          ]
        }
      ],
      "source": [
        "# Verify our sequences\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print(\"Input: \", text_from_ids(input_example))\n",
        "    print(\"--------\")\n",
        "    print(\"Target: \", text_from_ids(target_example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17azNHwUopYY",
        "outputId": "7f254360-d9f8-41dd-dc79-15bdace160b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 188,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Finally, we'll randomize the sequences so that we don't just memorize the books\n",
        "# in the order they were written, then build a new streaming dataset from that.\n",
        "# Using a streaming dataset allows us to pass the data to our network bit by bit,\n",
        "# rather than keeping it all in memory. We'll set it to figure out how much data\n",
        "# to prefetch in the background.\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "Qxlp1T6Nosuf"
      },
      "outputs": [],
      "source": [
        "class ConferenceTextModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units, dropout_rate):\n",
        "        super(ConferenceTextModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform',\n",
        "                                         dropout=dropout_rate)\n",
        "        self.lstm2 = tf.keras.layers.LSTM(rnn_units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform',\n",
        "                                         dropout=dropout_rate)\n",
        "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        if states is None:\n",
        "            states = self.lstm.get_initial_state(x)\n",
        "        x, state_h, state_c = self.lstm(x, initial_state=states, training=training)\n",
        "        x, state_h, state_c = self.lstm2(x, initial_state=[state_h, state_c], training=training)\n",
        "        x = self.batchnorm(x, training=training)  # Apply batch normalization\n",
        "        states = [state_h, state_c]\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else: \n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "FM4Y8BkSo-9_"
      },
      "outputs": [],
      "source": [
        "# Create an instance of our model\n",
        "vocab_size=len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "dropout_rate = 0.7\n",
        "\n",
        "model = ConferenceTextModel(vocab_size, embedding_dim, rnn_units, dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqMNqbN9pA4T",
        "outputId": "d5d35a6b-50e1-447d-8267-f30107c70d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 93) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Verify the output of our model is correct by running one sample through\n",
        "# This will also compile the model for us. This step will take a bit.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    states = [tf.zeros([64, model.lstm.units]), tf.zeros([64, model.lstm.units])]\n",
        "    example_batch_predictions = model(input_example_batch, states=states)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsJtjF-apEQK",
        "outputId": "eb9cea30-fd1e-41e4-dc7b-955ed93b46a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "6478/6478 [==============================] - 196s 30ms/step - loss: 1.1951 - val_loss: 0.9209\n",
            "Epoch 2/5\n",
            "6478/6478 [==============================] - 198s 30ms/step - loss: 0.9125 - val_loss: 0.7471\n",
            "Epoch 3/5\n",
            "6478/6478 [==============================] - 199s 31ms/step - loss: 0.7841 - val_loss: 0.6421\n",
            "Epoch 4/5\n",
            "6478/6478 [==============================] - 200s 31ms/step - loss: 0.7130 - val_loss: 0.5885\n",
            "Epoch 5/5\n",
            "6478/6478 [==============================] - 201s 31ms/step - loss: 0.6582 - val_loss: 0.5304\n"
          ]
        }
      ],
      "source": [
        "# # Define the loss function\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# initial_learning_rate = 0.1\n",
        "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate,\n",
        "#     decay_steps=100000,\n",
        "#     decay_rate=0.96,\n",
        "#     staircase=True)\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Assuming dataset is a tf.data.Dataset object\n",
        "train_size = int(0.8 * dataset.cardinality().numpy())\n",
        "val_size = int(0.2 * dataset.cardinality().numpy())\n",
        " \n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size).take(val_size)\n",
        " \n",
        "# Train the model with early stopping\n",
        "history = model.fit(dataset, epochs=5, validation_data=val_dataset, callbacks=[early_stopping_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "mjq4IP0ApH5R"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=0.5):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return chars_from_ids(predicted_ids), states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "J7O5T3cdpJXs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The world seemed like such a peaceful place until the magic tree was discovered in London. 28).\n",
            "\n",
            "\n",
            "\n",
            "Mormendogithinthindithandithinthithifofutheminevisthenditsthunsthendofuristhesthesatrivendofinthendithendounthithimendofurunthitevendithangrurthinthesthandithinthinthendoughthongivelasthoughthithedughendithithithithanthendurendendithalithinthinthinthenditsthingrurendofurithithendithendithinthithinthesthithendofuresthitherevelifffurthandourthalithalisthesthinsthiminthindithelalilinshthinthithendofinethilithithinthofrinevesthithendimndoulithesthomithinthatendithithinthakerendofurendogendurthithemithindoffalitrendithithendithendithendimengruplithanthathunthithithinthariofulitrinthindithendithinthithesthithesthalialithisthisinthithandithithithinthengrurendithitheralisthithesthithathalitrathasthithisitharilifffullinthshthandithenthionthisthathatimithithititeralisthinthalinthevinthithasthithithithealitithinthatethacresthathilishithithinthesthinthiofithithigagrasthithithaterathitalinthithanthasthatriorastithithsthithisasthalithasthatithesthithithesthonsithitheathanthinthaith\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of the character generator\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "# Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
        "# as its starting text\n",
        "states = None\n",
        "next_char = tf.constant(['The world seemed like such a peaceful place until the magic tree was discovered in London.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# Print the results formatted.\n",
        "print(result[0].numpy().decode('utf-8'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) states with unsupported characters which will be renamed to states_1 in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: layer_model\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: layer_model\\assets\n"
          ]
        }
      ],
      "source": [
        "model.save('layer_model', save_format='tf')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
