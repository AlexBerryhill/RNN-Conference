{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOOqznYzFl2gky2zLp7f5Qi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexBerryhill/RNN-Conference/blob/main/RNN_Conference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7_J02d99nN_1"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse Text"
      ],
      "metadata": {
        "id": "aVi3CaRIn47I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load file data\n",
        "path_to_file = tf.keras.utils.get_file('austen.txt', 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppt-qEtln4YB",
        "outputId": "bb6413ff-28d0-458c-afa8-3a8fde952117"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt\n",
            "4949262/4949262 [==============================] - 0s 0us/step\n",
            "Length of text: 4906787 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the first part of our data\n",
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KnUTK3jn_Dw",
        "outputId": "66d807ac-0018-43a8-c2ee-3a7f3450e516"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOLUME I\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings of\n",
            "existence; and had lived nearly twenty-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we'll get a list of the unique characters in the file. This will form the\n",
        "# vocabulary of our network. There may be some characters we want to remove from this\n",
        "# set as we refine the network.\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRixkBoZoLBr",
        "outputId": "642287a9-0e93-4277-b166-a3ed30b7e71a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86 unique characters\n",
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'é', '‘', '’', '“', '”']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we'll encode encode these characters into numbers so we can use them\n",
        "# with our neural network, then we'll create some mappings between the characters\n",
        "# and their numeric representations\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "\n",
        "# Here's a little helper function that we can use to turn a sequence of ids\n",
        "# back into a string:\n",
        "# turn them into a string:\n",
        "def text_from_ids(ids):\n",
        "  joinedTensor = tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "  return joinedTensor.numpy().decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "K68gIZiaoMq1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we'll verify that they work, by getting the code for \"A\", and then looking\n",
        "# that up in reverse\n",
        "testids = ids_from_chars([\"T\", \"r\", \"u\", \"t\", \"h\"])\n",
        "testids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv4hsydaoQTX",
        "outputId": "b8513ef1-439d-4721-9f7c-6100890e9a29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([45, 72, 75, 74, 62])>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids(testids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRZUQLiJoV7h",
        "outputId": "0e4e2b5c-f5d8-4151-89b1-9363f39093ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'T', b'r', b'u', b't', b'h'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testString = text_from_ids( testids )\n",
        "testString"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "agUgBQVqoXzc",
        "outputId": "126b7423-98c6-41d2-d2df-c585b8ad3d47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Truth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, create a stream of encoded integers from our text\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDtDGT06obfR",
        "outputId": "0aeeab82-0686-4cf5-aaec-f53d16f0aa15"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4906787,), dtype=int64, numpy=array([47, 40, 37, ..., 30, 39, 29])>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, convert that into a tensorflow dataset\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "UgVcMxsxod6A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, let's batch these sequences up into chunks for our training\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# This function will generate our sequence pairs:\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Call the function for every sequence in our list to create a new dataset\n",
        "# of input->target pairs\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "fGkC2bvvoffm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify our sequences\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print(\"Input: \", text_from_ids(input_example))\n",
        "    print(\"--------\")\n",
        "    print(\"Target: \", text_from_ids(target_example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgM632IdohE-",
        "outputId": "b82625a2-495d-4cb3-8f1b-c10426e31330"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  VOLUME I\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happ\n",
            "--------\n",
            "Target:  OLUME I\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we'll randomize the sequences so that we don't just memorize the books\n",
        "# in the order they were written, then build a new streaming dataset from that.\n",
        "# Using a streaming dataset allows us to pass the data to our network bit by bit,\n",
        "# rather than keeping it all in memory. We'll set it to figure out how much data\n",
        "# to prefetch in the background.\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17azNHwUopYY",
        "outputId": "6f9136e1-3d74-45fb-fd95-55d71e22b7c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our custom model. Given a sequence of characters, this\n",
        "# model's job is to predict what character should come next.\n",
        "class AustenTextModel(tf.keras.Model):\n",
        "\n",
        "  # This is our class constructor method, it will be executed when\n",
        "  # we first create an instance of the class\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "\n",
        "    # Our model will have three layers:\n",
        "\n",
        "    # 1. An embedding layer that handles the encoding of our vocabulary into\n",
        "    #    a vector of values suitable for a neural network\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # 2. A GRU layer that handles the \"memory\" aspects of our RNN. If you're\n",
        "    #    wondering why we use GRU instead of LSTM, and whether LSTM is better,\n",
        "    #    take a look at this article: https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm\n",
        "    #    then consider trying out LSTM instead (or in addition to!)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "    # 3. Our output layer that will give us a set of probabilities for each\n",
        "    #    character in our vocabulary.\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # This function will be executed for each epoch of our training. Here\n",
        "  # we will manually feed information from one layer of our network to the\n",
        "  # next.\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "\n",
        "    # 1. Feed the inputs into the embedding layer, and tell it if we are\n",
        "    #    training or predicting\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    # 2. If we don't have any state in memory yet, get the initial random state\n",
        "    #    from our GRUI layer.\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "\n",
        "    # 3. Now, feed the vectorized input along with the current state of memory\n",
        "    #    into the gru layer.\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "    # 4. Finally, pass the results on to the dense layer\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    # 5. Return the results\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "Qxlp1T6Nosuf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of our model\n",
        "vocab_size=len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = AustenTextModel(vocab_size, embedding_dim, rnn_units)"
      ],
      "metadata": {
        "id": "FM4Y8BkSo-9_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the output of our model is correct by running one sample through\n",
        "# This will also compile the model for us. This step will take a bit.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqMNqbN9pA4T",
        "outputId": "ab970f1d-9213-42b4-ddd3-904377a959b2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 87) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "history = model.fit(dataset, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsJtjF-apEQK",
        "outputId": "d8a69a6b-05f0-4fdf-ed3b-a06b1dd44dc2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "759/759 [==============================] - 49s 58ms/step - loss: 1.8336\n",
            "Epoch 2/20\n",
            "759/759 [==============================] - 45s 56ms/step - loss: 1.2210\n",
            "Epoch 3/20\n",
            "759/759 [==============================] - 46s 58ms/step - loss: 1.1289\n",
            "Epoch 4/20\n",
            "759/759 [==============================] - 44s 56ms/step - loss: 1.0820\n",
            "Epoch 5/20\n",
            "759/759 [==============================] - 45s 58ms/step - loss: 1.0477\n",
            "Epoch 6/20\n",
            "759/759 [==============================] - 46s 58ms/step - loss: 1.0180\n",
            "Epoch 7/20\n",
            "759/759 [==============================] - 45s 57ms/step - loss: 0.9909\n",
            "Epoch 8/20\n",
            "759/759 [==============================] - 45s 57ms/step - loss: 0.9664\n",
            "Epoch 9/20\n",
            "759/759 [==============================] - 45s 57ms/step - loss: 0.9443\n",
            "Epoch 10/20\n",
            "759/759 [==============================] - 46s 57ms/step - loss: 0.9247\n",
            "Epoch 11/20\n",
            "759/759 [==============================] - 45s 57ms/step - loss: 0.9085\n",
            "Epoch 12/20\n",
            "759/759 [==============================] - 46s 58ms/step - loss: 0.8952\n",
            "Epoch 13/20\n",
            "759/759 [==============================] - 46s 58ms/step - loss: 0.8849\n",
            "Epoch 14/20\n",
            "759/759 [==============================] - 46s 58ms/step - loss: 0.8774\n",
            "Epoch 15/20\n",
            "759/759 [==============================] - 45s 57ms/step - loss: 0.8715\n",
            "Epoch 16/20\n",
            "759/759 [==============================] - 46s 58ms/step - loss: 0.8685\n",
            "Epoch 17/20\n",
            "759/759 [==============================] - 45s 57ms/step - loss: 0.8671\n",
            "Epoch 18/20\n",
            "759/759 [==============================] - 45s 57ms/step - loss: 0.8663\n",
            "Epoch 19/20\n",
            "759/759 [==============================] - 46s 58ms/step - loss: 0.8678\n",
            "Epoch 20/20\n",
            "759/759 [==============================] - 44s 56ms/step - loss: 0.8697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's the code we'll use to sample for us. It has some extra steps to apply\n",
        "# the temperature to the distribution, and to make sure we don't get empty\n",
        "# characters in our text. Most importantly, it will keep track of our model\n",
        "# state for us.\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return chars_from_ids(predicted_ids), states\n"
      ],
      "metadata": {
        "id": "mjq4IP0ApH5R"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the character generator\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "# Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
        "# as its starting text\n",
        "states = None\n",
        "next_char = tf.constant(['Chapter 1'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# Print the results formatted.\n",
        "print(result[0].numpy().decode('utf-8'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7O5T3cdpJXs",
        "outputId": "8121cbc9-4bc5-442c-e0d9-8ad58624e862"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 10,\n",
            "\n",
            "“I obther things should not please.”\n",
            "\n",
            "“Mort is only the very memsing stanting with him, by one kiddly too,\n",
            "    “‘pport,\" tastiness, for it was all previously said,\n",
            "\n",
            "\"Oh, then,” cried her mother. “Fise well!”\n",
            "\n",
            "Elizabeth, affective is its being,” said the other a few faces at the door,\n",
            "would not be prevailed on, the lady was safely avariced, and Marianne\n",
            "would not juntified him, with an air of spirits, had she been away,\n",
            "and she had already said:\n",
            "\n",
            "“Decide no other to be in town less weech: but you would not be so, during\n",
            "whose indeeds, no prettier family tendency. I have known it probeits?\"\n",
            "\n",
            "\"I do, sir; but if it could see too much?”\n",
            "\n",
            "Four wedding day long; and Colonel Brandon had been giving offence of\n",
            "manners and invitation from Colonel Brandon.  His goodness of surprise and\n",
            "emotions, and proors at sea,\" said he, \"we will consider him in\n",
            "a hurry.\"\n",
            "\n",
            "\"There is Anne?\" waMks Mr. Collins cannot herself as far from impostible\n",
            "from sevense with the perfect brother of Edward, you know, kno\n"
          ]
        }
      ]
    }
  ]
}